{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612beae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numbers\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device for Mac\n",
    "# if torch.backends.mps.is_available():\n",
    "#         device = torch.device(\"mps\")\n",
    "#         print(device)\n",
    "\n",
    "# Setting device for Windows\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c585a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Resize((224)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83deb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224)),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n",
    "    transforms.RandomRotation(5, expand=False, center=None),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c676d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.convnext_base(pretrained=True)\n",
    "encoder.classifier[2]=nn.Linear(1024,number_of_classes)\n",
    "\n",
    "#path to the model trained in stage 1\n",
    "encoder.load_state_dict(torch.load('.pt'))\n",
    "encoder.classifier[2] = nn.Linear(1024, 1024)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a20587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_tensor, model = encoder):\n",
    "    model.eval()\n",
    "    features = model(image_tensor)\n",
    "    normalized_features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2825d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_smoothing(input_tensor, channels, sigma, kernel_size, dim):\n",
    "    if isinstance(kernel_size, numbers.Number):\n",
    "        kernel_size = [kernel_size] * dim\n",
    "    if isinstance(sigma, numbers.Number):\n",
    "        sigma = [sigma] * dim\n",
    "\n",
    "    kernel = 1\n",
    "    meshgrids = torch.meshgrid(\n",
    "        [torch.arange(size, dtype=torch.float32, device=input_tensor.device) for size in kernel_size]\n",
    "    )\n",
    "    for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "        mean = (size - 1) / 2\n",
    "        kernel *= 1 / (std * math.sqrt(2 * math.pi)) * torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "    kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "    kernel = kernel.view(1, 1, *kernel.size())\n",
    "    kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "    return F.conv2d(input_tensor, weight=kernel, groups=channels, padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2yuv_batch(images):\n",
    "    transformation_matrix = torch.tensor(\n",
    "        [[0.299, -0.14713, 0.615],\n",
    "         [0.587, -0.288862, -0.51499],\n",
    "         [0.114, 0.436, -0.10001]]\n",
    "    ).to(images.device)\n",
    "    \n",
    "    yuv_images = torch.tensordot(images, transformation_matrix, dims=([3], [0]))\n",
    "    return yuv_images\n",
    "\n",
    "def psnr_y_channel(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2, dim=[1, 2])\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse + 1e-10))\n",
    "    return psnr\n",
    "\n",
    "def psnr_hvsm_loss(img1, img2):\n",
    "    img1_yuv = rgb2yuv_batch(img1.permute(0, 2, 3, 1))\n",
    "    img2_yuv = rgb2yuv_batch(img2.permute(0, 2, 3, 1))\n",
    "\n",
    "    # Extract Y channel\n",
    "    img1_y = img1_yuv[:, :, :, 0]\n",
    "    img2_y = img2_yuv[:, :, :, 0]\n",
    "\n",
    "    psnr_values = psnr_y_channel(img1_y, img2_y)\n",
    "\n",
    "    return -torch.mean(psnr_values).to(img1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ce5a4-0b12-4d3a-a02e-856f321ccc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mifgsm(images, labels, eps, alpha, steps, decay, sigma, kernel_size, dim, target_features_dict,\n",
    "           min_psnr_hvsm = 40.5):\n",
    "    # Static variable to accumulate total steps\n",
    "    if not hasattr(mifgsm, \"total_steps\"):\n",
    "        mifgsm.total_steps = 0\n",
    "\n",
    "    images = images.clone().detach().to(device)\n",
    "    momentum = torch.zeros_like(images).detach().to(device)\n",
    "    adv_images = images.clone().detach()\n",
    "    \n",
    "    target_features = torch.stack([target_features_dict[label] for label in labels]).to(device)\n",
    "    \n",
    "    initial_psnr = -psnr_hvsm_loss(adv_images, images) #for psnr hvsm loss\n",
    "\n",
    "    for step in range(steps):\n",
    "        adv_images.requires_grad = True\n",
    "        \n",
    "        transformed_images = torch.stack([transform2(img) for img in adv_images])\n",
    "        smoothed_images = gaussian_smoothing(transformed_images, transformed_images.shape[1], sigma, kernel_size, dim)\n",
    "        \n",
    "        adv_features = extract_features(smoothed_images)\n",
    "        \n",
    "        similarity_loss = -torch.cosine_similarity(adv_features, target_features, dim=1).mean()\n",
    "        current_psnr = -psnr_hvsm_loss(adv_images, images)\n",
    "\n",
    "        psnr_loss = torch.abs(current_psnr - initial_psnr).mean().to(device)\n",
    "        #fractioned down psnr_loss to match similarity_loss in magnitude\n",
    "        total_loss = similarity_loss + (psnr_loss * 0.000001)\n",
    "        \n",
    "        grad = torch.autograd.grad(total_loss, adv_images, retain_graph=False, create_graph=False)[0]\n",
    "        \n",
    "        grad = grad / torch.norm(grad, p=2, dim=(1, 2, 3), keepdim=True)\n",
    "        grad = grad + decay * momentum\n",
    "        momentum = grad\n",
    "        \n",
    "        adv_images = adv_images.detach() + alpha * grad\n",
    "        delta = torch.clamp(adv_images - images, min=-eps, max=eps)\n",
    "        adv_images = torch.clamp(images + delta, min=0, max=1).detach()\n",
    "        \n",
    "        # Early stopping condition based on min_psnr_hvsm\n",
    "        if current_psnr <= min_psnr_hvsm:\n",
    "            mifgsm.total_steps += step + 1\n",
    "            break\n",
    "    \n",
    "    return adv_images, mifgsm.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(\n",
    "    source_dir, dest_dir,\n",
    "    target_features_dict,\n",
    "    alpha, epsilon, num_iter,\n",
    "    batch_size, kernel_size,\n",
    "    sigma, dim, decay):\n",
    "    \n",
    "    imgs = []\n",
    "    paths = []\n",
    "    labels = []  # To store labels for each image\n",
    "    \n",
    "    # Collect images, paths, and labels\n",
    "    for folder_name in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(image)\n",
    "            paths.append(img_path)\n",
    "            labels.append(folder_name)  # Use folder name as label\n",
    "    \n",
    "    idx = 0\n",
    "    with tqdm(total=len(imgs), desc=\"Processing Images\") as pbar:\n",
    "        while idx < len(imgs):\n",
    "            batch_imgs = imgs[idx:idx + batch_size]\n",
    "            batch_paths = paths[idx:idx + batch_size]\n",
    "            batch_labels = labels[idx:idx + batch_size]\n",
    "            idx += batch_size\n",
    "            tensor_imgs = torch.stack([transform(i) for i in batch_imgs]).to(device)\n",
    "           \n",
    "            adversarial_image_tensors, steps_taken = mifgsm(\n",
    "                tensor_imgs,\n",
    "                batch_labels,\n",
    "                epsilon,\n",
    "                alpha,\n",
    "                num_iter,\n",
    "                decay,\n",
    "                sigma,\n",
    "                kernel_size,\n",
    "                dim,\n",
    "                target_features_dict\n",
    "            )\n",
    "            \n",
    "            for adv_img, img_path in zip(adversarial_image_tensors, batch_paths):\n",
    "                adv_img = adv_img.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "                adv_img = np.clip(adv_img * 255, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                relative_path = os.path.relpath(os.path.dirname(img_path), source_dir)\n",
    "                dest_subfolder = os.path.join(dest_dir, relative_path)\n",
    "                if not os.path.exists(dest_subfolder):\n",
    "                    os.makedirs(dest_subfolder)\n",
    "                \n",
    "                filename = os.path.basename(img_path)[:-4]\n",
    "                dest_path = os.path.join(dest_subfolder, filename +'.png')\n",
    "                \n",
    "                cv2.imwrite(dest_path, cv2.cvtColor(adv_img, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            pbar.update(len(batch_imgs))\n",
    "    return steps_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ef813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramters used with recommended values\n",
    "alpha = 2/255\n",
    "eps = 8/255\n",
    "max_steps = 1000\n",
    "batch_size = 16\n",
    "kernel_size = 5\n",
    "sigma = 1.5\n",
    "dim = 2\n",
    "decay = 1.0\n",
    "steps_taken = 0\n",
    "\n",
    "#path to target_features computed in stage 1\n",
    "target_features_dict = torch.load('.pth')\n",
    "\n",
    "\n",
    "#path to source directory\n",
    "source_dir = ''\n",
    "\n",
    "#path to distination directory\n",
    "dest_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81212d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "steps_taken = process(source_dir, dest_dir, target_features_dict, alpha, eps, max_steps, batch_size, kernel_size, sigma, dim, decay)\n",
    "ed = time.time()\n",
    "print(f\"Total time taken is: {ed - st} seconds, steps taken {steps_taken}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
